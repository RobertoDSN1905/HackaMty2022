# -*- coding: utf-8 -*-
"""ModeloPredictivo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xSSZoO_4IwYnRrsH_dONs2dKbI63kRM9

**Instalar libreria seaborn**
"""

"""**Importaciones y checar versión de Tensorflow**"""

import pathlib
import io


import pandas as pd
import numpy as np
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from keras import layers
import time
print(tf.__version__)
archivo = open("Tetuan City power consumption.csv", "r")

raw_dataset = pd.read_csv(io.BytesIO(archivo))

dataset = raw_dataset.copy()

filtered_dataset = dataset.drop("DateTime", axis=1)
filtered_dataset.tail()

dataset.isna().sum()

dataset_entrenamiento = filtered_dataset.sample(frac=0.8, random_state=0)
dataset_prueba = filtered_dataset.drop(dataset_entrenamiento.index)

sns.pairplot(dataset_entrenamiento[["Zone 1 Power Consumption", "Zone 2  Power Consumption", "Zone 3  Power Consumption"]], diag_kind="kde")

estadisticas_entrenamiento =  dataset_entrenamiento.describe()
estadisticas_entrenamiento.pop("Zone 1 Power Consumption")
estadisticas_entrenamiento = estadisticas_entrenamiento.transpose()
estadisticas_entrenamiento

labels_entrenamiento = dataset_entrenamiento.pop("Zone 1 Power Consumption")
labels_prueba = dataset_prueba.pop("Zone 1 Power Consumption")

def norm(x):
  return (x - estadisticas_entrenamiento['mean']) / estadisticas_entrenamiento['std']

train_data_normalizada = norm(dataset_entrenamiento)
prueba_data_normalizada = norm(dataset_prueba)

from tensorflow.python.ops.nn_ops import relu
def construir_modelo():
  modelo = keras.Sequential([
      layers.Dense(64, activation=tf.nn.relu, input_shape = [len(dataset_entrenamiento.keys())]),
      layers.Dense(64, activation=tf.nn.relu),
      layers.Dense(1)
  ])

  optimizador = tf.keras.optimizers.RMSprop(0.001)

  modelo.compile(loss = 'mse', optimizer = optimizador, metrics =['mae', 'mse'])

  return modelo

modelo = construir_modelo()

"""Inspección del modelo"""

modelo.summary()

ejemplo = train_data_normalizada[:10]
resultadoEjemplo = modelo.predict(ejemplo)
resultadoEjemplo

class PrintDot(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs):
    if epoch % 100 == 0: print('')
    print('.', end = '')
EPOCHS = 2000
history = modelo.fit(train_data_normalizada, labels_entrenamiento, epochs = EPOCHS, validation_split = 0.2, verbose = 0, callbacks=[PrintDot()])

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()

import matplotlib.pyplot as plt

def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Error [Zona 1]')
  plt.plot(hist['epoch'], hist['mae'], label = 'Error de Entrenamiento')
  plt.plot(hist['epoch'], hist['val_mae'], label = 'Val Error')
  plt.legend()
  plt.ylim(0, 2000)

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Square Error')
  plt.plot(hist['epoch'], hist['mae'], label = 'Error de entrenamiento')
  plt.plot(hist['epoch'], hist['val_mse'], label = 'Val Error')
  plt.legend()
  plt.ylim([0,1500])

plot_history(history)

loss, mae, mse = modelo.evaluate(prueba_data_normalizada, labels_prueba, verbose=0)

print("Error de prueba absoluto promedio: {:5.2f} kWh".format(mae))

"""Error total equivalente a 67 pesos de perdida en el peor de los casos"""

test_prediction = modelo.predict(prueba_data_normalizada).flatten()

plt.scatter(labels_prueba, test_prediction)
plt.xlabel('Valores verdaderos')
plt.ylabel('Valores predecidos')

plt.axis('equal')
plt.axis('square')

plt.xlim([0, plt.xlim()[1]])
plt.ylim([0, plt.ylim()[1]])

_=plt.plot([-1000, 1000], [-1000, 1000])

error = test_prediction - labels_prueba
plt.hist(error, bins = 25)
plt.xlabel("Predicción de error kWh")
_=plt.ylabel("Count")